\chapter{Data Aggregation}

The biggest initial task for this thesis was the acquisition of data.
The data should be as extensive as possible, feature a high conjunction between contributers over several repositories to verify a possible connection between those and have realistic meta data.
Two different solutions approaches existed for these requirements.

The first approach was to design an algorithm for automatic local generation of repositories.
The main problem with this solution is, that the visualization and data mining code might be highly optimized for this specific generation algorithm.
Real world data is noisy and inconsistent. Thereby the developed solution might have worked on the generated data, but would have probably failed on real world data.

The second solution was to get real world data from somewhere. The most obvious choice was to mine data from open source projects.
I chose Github for this purpose, as it hosts one of the biggest collection of open source projects and provides a great API for querying Github's meta data.
A problem with this approach is that we don't have access to all important meta data, as for example the full list of members for organizations or the internal team structure of organizations.
Another problem are old email addresses, which are not related to any account anymore.
Even though some ground truth is missing, I decided to use this approach as it was still the most viable and promising way to gain as much ground truth and real world noise as possible.


\section{Structure of the Data}

Before we get to the data aggregator, I want to explain the internal git storage and meta data structure and mechanisms, which are important for the purpose of this thesis.


Test~\cite{book:pro-git}.


\section{Problems}


\section{The Aggregator}


